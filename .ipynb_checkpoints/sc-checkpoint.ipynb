{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libraries.pickleFunctions import writeObjToPickle,loadPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model\n",
    "We trained a word2vec model on wikipedia corpus and have it saved.(code can be found in wikipedia_full_train.py file). We load this model and set up variables like vocabulary dict, embedding size, unknown word representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model():\n",
    "    model = Word2Vec.load(\"/home/rohit.katariya/rohit/sentence_classification/datadump/wiki_full_model.text.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocabulary=w2v_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_embeddings_vector = w2v_model.trainables.layer1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_word_rep = [0.0]* size_embeddings_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We read the dataset of question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename=\"datadump/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Sentences to Embeddings\n",
    "We write a function to be used later to convert a sentence to its corresponding word2vec representation, padding with *unknown* word representation in front in case it is smaller than the max size of sentence fixed. We also truncate very large sentences(this was very small 0.02%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embedding_to_sentence(sentence,max_sentence_len):\n",
    "    sentence= str(sentence)\n",
    "    sentence_words = sentence.split()\n",
    "    sentence_embeddings = []\n",
    "    for sentence_word in sentence_words:\n",
    "        if sentence_word in w2v_vocabulary:\n",
    "            sentence_embeddings.append(w2v_model.wv.__getitem__(sentence_word))\n",
    "        else:\n",
    "            sentence_embeddings.append(unk_word_rep)\n",
    "    sentence_embeddings = sentence_embeddings[:max_sentence_len]\n",
    "    sentence_embeddings = [unk_word_rep]*(max_sentence_len - len(sentence_embeddings)) + sentence_embeddings\n",
    "    return  sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix maximum length that a sentence can take up\n",
    "\n",
    "We find the maximum length a sentence a sentence can take and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_len(sen):\n",
    "    sen= str(sen)\n",
    "    return len(sen.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_each_1 = data_df[\"question1\"].apply(get_sentence_len)\n",
    "len_each_2 = data_df[\"question2\"].apply(get_sentence_len)\n",
    "len_each = pd.concat([len_each_1,len_each_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    808580.000000\n",
       "mean         11.062100\n",
       "std           5.884595\n",
       "min           1.000000\n",
       "25%           7.000000\n",
       "50%          10.000000\n",
       "75%          13.000000\n",
       "max         237.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_each.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = int(len_each.quantile(0.998))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply embedding transformations to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"question1_emb\"]=data_df.question1.apply(apply_embedding_to_sentence,max_sentence_len=max_sentence_len)\n",
    "data_df[\"question2_emb\"]=data_df.question2.apply(apply_embedding_to_sentence,max_sentence_len=max_sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training, testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_processed_X = data_df[['question1_emb','question2_emb']]\n",
    "data_df_processed_Y = data_df[['is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_df_processed_X, data_df_processed_Y, test_size=5000*1.0/len(data_df_processed_X), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train test split: [399290, 5000]\n"
     ]
    }
   ],
   "source": [
    "print( \"length of train test split:\" ,[len(s) for s in [y_train, y_test] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pre-processed data to disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training test set to disc so that we don't have to reload full word2vec model every time we want to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_pickle(\"datadump/preprocessed_data/xtrain.pkl\")\n",
    "x_test.to_pickle(\"datadump/preprocessed_data/x_test.pkl\")\n",
    "y_train.to_pickle(\"datadump/preprocessed_data/y_train.pkl\")\n",
    "y_test.to_pickle(\"datadump/preprocessed_data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def writeObjToPickle(dict_,filename):\n",
    "    \"\"\"\n",
    "        writes an object to pickle file named filename.\n",
    "    \"\"\"\n",
    "    file_dict = open(filename+'.pickle', 'wb')\n",
    "    pickle.dump(dict_, file_dict)\n",
    "    file_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sentence_len\n",
    "# from libraries.pickleFunctions import writeObjToPickle,loadPickle\n",
    "writeObjToPickle( max_sentence_len,\"datadump/preprocessed_data/max_sentence_len\")\n",
    "writeObjToPickle( size_embeddings_vector,\"datadump/preprocessed_data/size_embeddings_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing preprocessed data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done writing preprocessed data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "We saved the sentence data and now we can start loading these from here. We load the train,test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libraries.pickleFunctions import writeObjToPickle,loadPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"datadump/preprocessed_data/xtrain.pkl\")\n",
    "x_test = pd.read_pickle(\"datadump/preprocessed_data/x_test.pkl\")\n",
    "y_train = pd.read_pickle(\"datadump/preprocessed_data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"datadump/preprocessed_data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn import preprocessing\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# lb.fit([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_y = lb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([ f for f in tr_y if f == [1]]),len([ f for f in tr_y if f == [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train test split: [399290, 5000]\n"
     ]
    }
   ],
   "source": [
    "print( \"length of train test split:\" ,[len(s) for s in [y_train, y_test] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = loadPickle(\"datadump/preprocessed_data/max_sentence_len.pickle\")\n",
    "size_embeddings_vector = loadPickle(\"datadump/preprocessed_data/size_embeddings_vector.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into batches\n",
    "We convert data into batches of batch_size sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "display=1\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(batch_size,df_X, df_Y):\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(df_Y)\n",
    "    for start_i in range(0, len(df_X), batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [df_X[start_i:end_i], df_Y[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_into_batches(pd_df,batch_size):\n",
    "#     return np.split(pd_df, len(pd_df)/(batch_size+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = split_into_batches(batch_size,x_train, y_train)\n",
    "testing_batches = split_into_batches(batch_size,x_test, y_test)\n",
    "# batch_x1_list, batch_x2_list, batch_y_list = np.split(x_train.question1_emb, len(x_train.question1_emb)/128) ,x_train.question2_emb, y_train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outout_batches[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = size_embeddings_vector\n",
    "n_steps = max_sentence_len \n",
    "n_hidden = 128 \n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs],name=\"sentence_1\")\n",
    "sentence_2 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs],name=\"sentence_2\")\n",
    "y = tf.placeholder(tf.int64, shape=[None],name = 'y')\n",
    "keep_prob = tf.placeholder(tf.float32,name =\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sentence_tensor(sentence_tf,n_steps):\n",
    "    # input: sentence_tf: tensor of shape [batch_size, n_steps, n_inputs]\n",
    "    #output: sentence_tf_new: list of tensors with dimensions [batch_size, n_inputs]; len of this list n_steps\n",
    "    # first we bring the second dimension n_steps in front\n",
    "    transposed_tf = tf.transpose(sentence_tf, [1, 0, 2])   #[n_steps, batch_size, n_inputs]\n",
    "    # we then split the tensor into n_step tensors\n",
    "    splitted_tf_list = tf.split(transposed_tf,axis=0,num_or_size_splits=n_steps)  # list of tensors [1,batch_size, n_inputs]\n",
    "    # now we will remove the first dimension from each tensor using squeez function\n",
    "    squezed_tf_list = [tf.squeeze(splitted_tf, [0]) for splitted_tf in  splitted_tf_list] #list of tensors [batch_size, n_inputs]\n",
    "    return squezed_tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1_list =reshape_sentence_tensor(sentence_1,n_steps)\n",
    "sentence_2_list =reshape_sentence_tensor(sentence_2,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Architecture\n",
    "Now that we have data in the format required by LSTM, we can start building our Siamese LSTM architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM Cells\n",
    "First we create 2 LSTM cells of the bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_lstm_cell  = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "backward_lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Insert dropout layer to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(forward_lstm_cell, output_keep_prob=keep_prob)\n",
    "backward_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(backward_lstm_cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope('siamese_network') as scope:\n",
    "#     with tf.name_scope('Bi_LSTM_1'):\n",
    "#         _, last_state_fw1, last_state_bw1 = tf.nn.bidirectional_rnn(\n",
    "#                                         lstm_fw_cell, lstm_bw_cell, x1_,\n",
    "#                                         dtype=tf.float32)\n",
    "#     with tf.name_scope('Bi_LSTM_2'):\n",
    "#         scope.reuse_variables() # tied weights (reuse the weights from `Bi_LSTM_1` for `Bi_LSTM_2`)\n",
    "#         _, last_state_fw2, last_state_bw2 = tf.nn.bidirectional_rnn(\n",
    "#                                         lstm_fw_cell, lstm_bw_cell, x2_,\n",
    "#                                         dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"siamese_network\") as scope:\n",
    "    _, (last_state_fw1,last_state_bw1) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_lstm_cell, \n",
    "                                                                         cell_bw=backward_lstm_cell, \n",
    "                                                                         inputs = sentence_1,\n",
    "                                                                         dtype=tf.float32)\n",
    "with tf.variable_scope(scope, reuse=True):\n",
    "  _, (last_state_fw2, last_state_bw2) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_lstm_cell, \n",
    "                                                                         cell_bw=backward_lstm_cell, \n",
    "                                                                         inputs = sentence_2,\n",
    "                                                                         dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_state(fw_state,bw_state):\n",
    "    return tf.concat([fw_state[0],fw_state[1],bw_state[0],bw_state[1] ],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rep_out = get_rep_state(last_state_fw1,last_state_bw1)\n",
    "s2_rep_out = get_rep_state(last_state_fw2,last_state_bw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_distance = tf.exp( -tf.reduce_sum(tf.abs(s1_rep_out - s2_rep_out),1))\n",
    "logits = [ man_distance ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Exp_1:0' shape=(?,) dtype=float32>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Exp_1:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels =y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), y) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network training begins.\n",
      "iteration: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 1) for Tensor 'y:0', which has shape '(?,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6e92501ca93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion2_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mly\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/analytics/rohit/vPy/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/analytics/rohit/vPy/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1105\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 1) for Tensor 'y:0', which has shape '(?,)'"
     ]
    }
   ],
   "source": [
    "init = tf. global_variables_initializer()\n",
    "max_iter = 4\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print('Network training begins.')\n",
    "    for i in range(1, max_iter + 1):\n",
    "        print(\"iteration:\",i)\n",
    "        for batch_X, batch_Y in training_batches:\n",
    "#             sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "            batch_x1, batch_x2, batch_y = list(batch_X.question1_emb),list(batch_X.question2_emb),  list(batch_Y.is_duplicate) \n",
    "            feed_dict = {sentence_1: batch_x1, sentence_2: batch_x2, y: batch_y, keep_prob: .9}\n",
    "            _, loss_, accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "        \n",
    "        if i % display == 0:\n",
    "            print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, loss_, accuracy_))\n",
    "        \n",
    "#         # Testing the network\n",
    "#         if i % n_test == 0:\n",
    "#             # Retrieving data from the test set\n",
    "#             batch_x1, batch_x2, batch_y = list(x_test.question1_emb),list(x_test.question2_emb), list(y_test.is_duplicate)\n",
    "#             feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: 1.0}\n",
    "#             accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "#             print('testing step %i, accuracy %.3f' % (i, accuracy_test))\n",
    "#     print('********************************')\n",
    "#     print('Training finished.')\n",
    "    \n",
    "#     # testing the trained network on a large sample\n",
    "#     batch_x1, batch_x2, batch_y = list(x_test.question1_emb),list(x_test.question2_emb), list(y_test.is_duplicate)\n",
    "#     feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob:1.0}\n",
    "#     accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "#     print('********************************')\n",
    "#     print('Testing the network.')\n",
    "#     print('Network accuracy %.3f' % (accuracy_test))\n",
    "#     print('********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
