{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libraries.pickleFunctions import writeObjToPickle,loadPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model\n",
    "We trained a word2vec model on wikipedia corpus and have it saved.(code can be found in wikipedia_full_train.py file). We load this model and set up variables like vocabulary dict, embedding size, unknown word representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model():\n",
    "    model = Word2Vec.load(\"/home/rohit.katariya/rohit/sentence_classification/datadump/wiki_full_model.text.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocabulary=w2v_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_embeddings_vector = w2v_model.trainables.layer1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_word_rep = [0.0]* size_embeddings_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We read the dataset of question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename=\"datadump/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Sentences to Embeddings\n",
    "We write a function to be used later to convert a sentence to its corresponding word2vec representation, padding with *unknown* word representation in front in case it is smaller than the max size of sentence fixed. We also truncate very large sentences(this was very small 0.02%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embedding_to_sentence(sentence,max_sentence_len):\n",
    "    sentence= str(sentence)\n",
    "    sentence_words = sentence.split()\n",
    "    sentence_embeddings = []\n",
    "    for sentence_word in sentence_words:\n",
    "        if sentence_word in w2v_vocabulary:\n",
    "            sentence_emb = w2v_model.wv.__getitem__(sentence_word)\n",
    "            sentence_embeddings.append( np.asarray(sentence_emb) )\n",
    "        else:\n",
    "            sentence_embeddings.append(unk_word_rep)\n",
    "    sentence_embeddings = sentence_embeddings[:max_sentence_len]\n",
    "    sentence_embeddings = [unk_word_rep]*(max_sentence_len - len(sentence_embeddings)) + sentence_embeddings\n",
    "    return  np.asarray( sentence_embeddings )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix maximum length that a sentence can take up\n",
    "\n",
    "We find the maximum length a sentence a sentence can take and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_len(sen):\n",
    "    sen= str(sen)\n",
    "    return len(sen.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_each_1 = data_df[\"question1\"].apply(get_sentence_len)\n",
    "len_each_2 = data_df[\"question2\"].apply(get_sentence_len)\n",
    "len_each = pd.concat([len_each_1,len_each_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    808580.000000\n",
       "mean         11.062100\n",
       "std           5.884595\n",
       "min           1.000000\n",
       "25%           7.000000\n",
       "50%          10.000000\n",
       "75%          13.000000\n",
       "max         237.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_each.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = int(len_each.quantile(0.998))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply embedding transformations to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"question1_emb\"]=data_df.question1.apply(apply_embedding_to_sentence,max_sentence_len=max_sentence_len)\n",
    "data_df[\"question2_emb\"]=data_df.question2.apply(apply_embedding_to_sentence,max_sentence_len=max_sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training, testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_processed_X = data_df[['question1_emb','question2_emb']]\n",
    "data_df_processed_Y = data_df[['is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_df_processed_X, data_df_processed_Y, test_size=5000*1.0/len(data_df_processed_X), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train test split: [399290, 5000]\n"
     ]
    }
   ],
   "source": [
    "print( \"length of train test split:\" ,[len(s) for s in [y_train, y_test] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399290, 44, 50)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(list(x_train.question1_emb)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pre-processed data to disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training test set to disc so that we don't have to reload full word2vec model every time we want to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_pickle(\"datadump/preprocessed_data/xtrain.pkl\")\n",
    "x_test.to_pickle(\"datadump/preprocessed_data/x_test.pkl\")\n",
    "y_train.to_pickle(\"datadump/preprocessed_data/y_train.pkl\")\n",
    "y_test.to_pickle(\"datadump/preprocessed_data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# def writeObjToPickle(dict_,filename):\n",
    "#     \"\"\"\n",
    "#         writes an object to pickle file named filename.\n",
    "#     \"\"\"\n",
    "#     file_dict = open(filename+'.pickle', 'wb')\n",
    "#     pickle.dump(dict_, file_dict)\n",
    "#     file_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sentence_len\n",
    "# from libraries.pickleFunctions import writeObjToPickle,loadPickle\n",
    "writeObjToPickle( max_sentence_len,\"datadump/preprocessed_data/max_sentence_len\")\n",
    "writeObjToPickle( size_embeddings_vector,\"datadump/preprocessed_data/size_embeddings_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing preprocessed data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done writing preprocessed data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "We saved the sentence data and now we can start loading these from here. We load the train,test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libraries.pickleFunctions import writeObjToPickle,loadPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"datadump/preprocessed_data/xtrain.pkl\")\n",
    "x_test = pd.read_pickle(\"datadump/preprocessed_data/x_test.pkl\")\n",
    "y_train = pd.read_pickle(\"datadump/preprocessed_data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"datadump/preprocessed_data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_transform(lables_sample):\n",
    "    ohe_vec = np.zeros( (len(lables_sample),2), dtype=np.int)\n",
    "    for x_ind in range(len(lables_sample)):\n",
    "        ohe_vec[x_ind] = np.zeros(2, dtype=np.int)\n",
    "        ohe_vec[x_ind][lables_sample[x_ind]] = 1\n",
    "    ohe_vec_dict = {}\n",
    "    for k in range(len(ohe_vec)):\n",
    "        ohe_vec_dict[k] = np.asarray( ohe_vec[k])\n",
    "    return ohe_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dict = bin_transform([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_transformed = y_train.is_duplicate.apply(transform_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_transformed = y_test.is_duplicate.apply(transform_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train test split: [399290, 5000]\n"
     ]
    }
   ],
   "source": [
    "print( \"length of train test split:\" ,[len(s) for s in [y_train, y_test] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = loadPickle(\"datadump/preprocessed_data/max_sentence_len.pickle\")\n",
    "size_embeddings_vector = loadPickle(\"datadump/preprocessed_data/size_embeddings_vector.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into batches\n",
    "We convert data into batches of batch_size sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "display=1\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(batch_size,df_X, df_Y):\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(df_Y)\n",
    "    for start_i in range(0, len(df_X), batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [df_X[start_i:end_i], df_Y[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_into_batches(pd_df,batch_size):\n",
    "#     return np.split(pd_df, len(pd_df)/(batch_size+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = split_into_batches(batch_size,x_train, y_train_transformed)\n",
    "testing_batches = split_into_batches(batch_size,x_test, y_train_transformed)\n",
    "# batch_x1_list, batch_x2_list, batch_y_list = np.split(x_train.question1_emb, len(x_train.question1_emb)/128) ,x_train.question2_emb, y_train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_batches[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = size_embeddings_vector\n",
    "n_steps = max_sentence_len \n",
    "n_hidden = 128 \n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs],name=\"sentence_1\")\n",
    "sentence_2 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs],name=\"sentence_2\")\n",
    "y = tf.placeholder(tf.int64, shape=[None,2],name = 'y')\n",
    "keep_prob = tf.placeholder(tf.float32,name =\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sentence_tensor(sentence_tf,n_steps):\n",
    "    # input: sentence_tf: tensor of shape [batch_size, n_steps, n_inputs]\n",
    "    #output: sentence_tf_new: list of tensors with dimensions [batch_size, n_inputs]; len of this list n_steps\n",
    "    # first we bring the second dimension n_steps in front\n",
    "    transposed_tf = tf.transpose(sentence_tf, [1, 0, 2])   #[n_steps, batch_size, n_inputs]\n",
    "    # we then split the tensor into n_step tensors\n",
    "    splitted_tf_list = tf.split(transposed_tf,axis=0,num_or_size_splits=n_steps)  # list of tensors [1,batch_size, n_inputs]\n",
    "    # now we will remove the first dimension from each tensor using squeez function\n",
    "    squezed_tf_list = [tf.squeeze(splitted_tf, [0]) for splitted_tf in  splitted_tf_list] #list of tensors [batch_size, n_inputs]\n",
    "    return squezed_tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1_list =reshape_sentence_tensor(sentence_1,n_steps)\n",
    "sentence_2_list =reshape_sentence_tensor(sentence_2,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Architecture\n",
    "Now that we have data in the format required by LSTM, we can start building our Siamese LSTM architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM Cells\n",
    "First we create 2 LSTM cells of the bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_lstm_cell  = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "backward_lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Insert dropout layer to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(forward_lstm_cell, output_keep_prob=keep_prob)\n",
    "backward_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(backward_lstm_cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_fw = initial_state_fw = forward_lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "# state_bw = initial_state_bw = backward_lstm_cell.zero_state(batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope('siamese_network') as scope:\n",
    "#     with tf.name_scope('Bi_LSTM_1'):\n",
    "#         _, last_state_fw1, last_state_bw1 = tf.nn.bidirectional_rnn(\n",
    "#                                         lstm_fw_cell, lstm_bw_cell, x1_,\n",
    "#                                         dtype=tf.float32)\n",
    "#     with tf.name_scope('Bi_LSTM_2'):\n",
    "#         scope.reuse_variables() # tied weights (reuse the weights from `Bi_LSTM_1` for `Bi_LSTM_2`)\n",
    "#         _, last_state_fw2, last_state_bw2 = tf.nn.bidirectional_rnn(\n",
    "#                                         lstm_fw_cell, lstm_bw_cell, x2_,\n",
    "#                                         dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"siamese_network\") as scope:\n",
    "    _, (last_state_fw1,last_state_bw1) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_lstm_cell, \n",
    "                                                                         cell_bw=backward_lstm_cell, \n",
    "                                                                         inputs = sentence_1,\n",
    "                                                                         dtype=tf.float32)\n",
    "with tf.variable_scope(scope, reuse=True):\n",
    "  _, (last_state_fw2, last_state_bw2) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_lstm_cell, \n",
    "                                                                         cell_bw=backward_lstm_cell, \n",
    "                                                                         inputs = sentence_2,\n",
    "                                                                         dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_state(fw_state,bw_state):\n",
    "    return tf.concat([fw_state[0],fw_state[1],bw_state[0],bw_state[1] ],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rep_out = get_rep_state(last_state_fw1,last_state_bw1)\n",
    "s2_rep_out = get_rep_state(last_state_fw2,last_state_bw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y:0' shape=(?, 2) dtype=int64>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.get_variable('weigths_out', shape=[4 * n_hidden, n_classes],\n",
    "                initializer=tf.random_normal_initializer(stddev=1.0/float(n_hidden)))\n",
    "biases = tf.get_variable('biases_out', shape=[n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_2:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), y) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-eadc04546a95>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels =y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network training begins.\n",
      "iteration: 1\n",
      "step 1, training loss: 0.69315, training accuracy: 0.552\n",
      "iteration: 2\n"
     ]
    }
   ],
   "source": [
    "init = tf. global_variables_initializer()\n",
    "max_iter = 4\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print('Network training begins.')\n",
    "    for i in range(1, max_iter + 1):\n",
    "        print(\"iteration:\",i)\n",
    "        for batch_X, batch_Y in training_batches:\n",
    "#             sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "            batch_x1, batch_x2, batch_y = np.asarray(list(batch_X.question1_emb)),np.asarray(list(batch_X.question2_emb)), np.asarray(list(batch_Y)) \n",
    "            feed_dict = {sentence_1: batch_x1, sentence_2: batch_x2, y: batch_y, keep_prob: .9}\n",
    "            _, loss_, accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "#             print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, loss_, accuracy_))\n",
    "        \n",
    "        if i % display == 0:\n",
    "            print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, loss_, accuracy_))\n",
    "#         # Testing the network\n",
    "#         if i % n_test == 0:\n",
    "#             # Retrieving data from the test set\n",
    "#             batch_x1, batch_x2, batch_y = list(x_test.question1_emb),list(x_test.question2_emb), list(y_test.is_duplicate)\n",
    "#             feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: 1.0}\n",
    "#             accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "#             print('testing step %i, accuracy %.3f' % (i, accuracy_test))\n",
    "#     print('********************************')\n",
    "#     print('Training finished.')\n",
    "    \n",
    "#     # testing the trained network on a large sample\n",
    "#     batch_x1, batch_x2, batch_y = list(x_test.question1_emb),list(x_test.question2_emb), list(y_test.is_duplicate)\n",
    "#     feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob:1.0}\n",
    "#     accuracy_test = sess.run(accuracy, feed_dict=feed_dict)\n",
    "#     print('********************************')\n",
    "#     print('Testing the network.')\n",
    "#     print('Network accuracy %.3f' % (accuracy_test))\n",
    "#     print('********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_X.question1_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray(list(batch_Y)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray( list(batch_X.question1_emb)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf. global_variables_initializer()\n",
    "# max_iter = 4\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     print('Network training begins.')\n",
    "#     for i in range(1, max_iter + 1):\n",
    "#         print(\"iteration:\",i)\n",
    "#         for batch_X, batch_Y in training_batches:\n",
    "# #             sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "#             batch_x1, batch_x2, batch_y = np.asarray(list(batch_X.question1_emb)),np.asarray(list(batch_X.question2_emb)), np.asarray(list(batch_Y)) \n",
    "#             feed_dict = {sentence_1: batch_x1, sentence_2: batch_x2, y: batch_y, keep_prob: .9}\n",
    "#             _, loss_ = sess.run([optimizer, loss ], feed_dict=feed_dict)\n",
    "        \n",
    "#         if i % display == 0:\n",
    "#             print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, loss_, accuracy_))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
